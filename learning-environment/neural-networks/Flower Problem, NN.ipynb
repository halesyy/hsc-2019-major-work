{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data defined\n",
    "\n",
    "Where index is as follows: the length, width and where it's weight should be defined.\n",
    "\n",
    "0 = blue,\n",
    "1 = red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [3, 1.5, 1],\n",
    "    [2, 1, 0],\n",
    "    [4, 1.5, 1],\n",
    "    [3, 1, 0],\n",
    "    [3.5, 0.5, 1],\n",
    "    [2, 0.5, 0],\n",
    "    [5.5, 1, 1],\n",
    "    [1, 1, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN is our Neural Network function\n",
    "\n",
    "Accepting matching data, with their weight application and bias generated later in numpy's random number generator. Also defining sigmoid, an expo function which packs any data into values between 0 and 1, negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(m1, m2, w1, w2, b):\n",
    "    z = m1 * w1 + m2 * w2 + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + numpy.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights():\n",
    "    return [numpy.random.randn(), numpy.random.randn(), numpy.random.randn()]\n",
    "\n",
    "w1, w2, b = random_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an example, with previously assigned random weights and bias\n",
    "\n",
    "We are running this for our data set's first set of data:\n",
    "\n",
    "**A red flower (val 1) which has a length 3 and height 1.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00412091539736348 blue\n",
      "but is really red\n"
     ]
    }
   ],
   "source": [
    "flower     = 0 # the flower that we choose to train with\n",
    "\n",
    "\n",
    "prediction = NN(data[flower][0], data[flower][1], w1, w2, b)\n",
    "target     = data[flower][2]\n",
    "\n",
    "\n",
    "print(prediction, \"red\" if prediction >= 0.5 else \"blue\") \n",
    "print(\"but is really {0}\".format(\"red\" if target >= 0.5 else \"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (Squared Error Cost Function)\n",
    "\n",
    "Squaring the difference between the target (or real) and the prediction, helps us get an idea of how to minimize the cost and get better predictions.\n",
    "\n",
    "**When the slope is positive** it means that there must be subtracted a positive integer to get closer to the zero approximation (once at 0, the cost function is at it's equalibrium), on the other side **when the slope is negative** that means it is approaching the 0-point, so it must be subtracted by a negative number.\n",
    "\n",
    "### Note that prediction is the only value which changes, so that is our \"goal\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost equation for the most recent prediction is: 0.703798011268404\n",
      ", With a slope of 0.5590593285124029\n"
     ]
    }
   ],
   "source": [
    "# cost function to tell the computer how far out it was, lower = better\n",
    "def cost(prediction, target):\n",
    "    return (prediction - target) ** 2\n",
    "\n",
    "# getting the slope of this function, as it requires tweaking for backwards integration\n",
    "def slope(prediction, target):\n",
    "    return 2 ** (prediction - target) * 1\n",
    "\n",
    "c1 = cost(prediction, target)\n",
    "s1 = slope(prediction, target)\n",
    "\n",
    "print(\"The cost equation for the most recent prediction is: {0}\".format(c1))\n",
    "print(\", With a slope of {0}\".format(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the slope into a closer depiction of the point value\n",
    "\n",
    "To do this: positive slopes require subtracting a positive float, and for negative slopes, subtracting negative floats. So, to recap, the slope function is the differential (gradient finder) for our cost function - the **cost function** being the prime way of measuring whether or not our computer is doing well in the ability to guess the colour from given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
