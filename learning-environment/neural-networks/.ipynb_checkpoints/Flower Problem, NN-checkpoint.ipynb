{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data defined\n",
    "\n",
    "Where index is as follows: the length, width and where it's weight should be defined.\n",
    "\n",
    "0 = blue,\n",
    "1 = red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = [\n",
    "    [3, 1.5, 1],\n",
    "    [2, 1, 0],\n",
    "    [4, 1.5, 1],\n",
    "    [3, 1, 0],\n",
    "    [3.5, 0.5, 1],\n",
    "    [2, 0.5, 0],\n",
    "    [5.5, 1, 1], \n",
    "    [4.4, 1, \"?\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN is our Neural Network function\n",
    "\n",
    "Accepting matching data, with their weight application and bias generated later in numpy's random number generator. Also defining sigmoid, an expo function which packs any data into values between 0 and 1, negative or positive. M = Measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed(m1, m2, w1, w2, b):\n",
    "    z = m1 * w1 + m2 * w2 + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + numpy.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, b = 7.526382310815231, 3.4870577682723902, -26.253997846696628"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an example, with previously assigned random weights and bias\n",
    "\n",
    "We are running this for our data set's first set of data:\n",
    "\n",
    "**A red flower (val 1) which has a length 3 and height 1.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red = red\n",
      "blue = blue\n",
      "red = red\n",
      "blue = blue\n",
      "red = red\n",
      "blue = blue\n",
      "red = red\n",
      "Model: 100.0% effective, overall 7/7\n"
     ]
    }
   ],
   "source": [
    "def run_model_predictions():\n",
    "    for idx in range(len(flowers)):\n",
    "        prediction = feed(flowers[idx][0], flowers[idx][1], w1, w2, b)\n",
    "        target     = flowers[idx][2]\n",
    "        print(\"{0} = {1}\".format(\"red\" if prediction >= 0.5 else \"blue\", \"red\" if target >= 0.5 else \"blue\"))\n",
    "\n",
    "def predict_from_m(m1, m2):\n",
    "    prediction = feed(m1, m2, w1, w2, b)\n",
    "    target     = \"?\"\n",
    "    print(\"{0} = {1}\".format(\"red\" if prediction >= 0.5 else \"blue\", \"?\"))\n",
    "\n",
    "\n",
    "def model():\n",
    "    correct = 0\n",
    "    for i in range(len(flowers)):\n",
    "        prediction = 1 if feed(flowers[i][0], flowers[i][1], w1, w2, b) >= 0.5 else 0\n",
    "#         print(prediction)\n",
    "        if flowers[i][2] == prediction:\n",
    "            correct += 1\n",
    "    print(\"Model: {0}% effective, overall {1}/{2}\".format(round(correct/len(flowers)*100, 2), correct, len(flowers)))\n",
    "       \n",
    "\n",
    "run_model_predictions()\n",
    "model()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (Squared Error Cost Function)\n",
    "\n",
    "Squaring the difference between the target (or real) and the prediction, helps us get an idea of how to minimize the cost and get better predictions.\n",
    "\n",
    "**When the slope is positive** it means that there must be subtracted a positive integer to get closer to the zero approximation (once at 0, the cost function is at it's equalibrium), on the other side **when the slope is negative** that means it is approaching the 0-point, so it must be subtracted by a negative number.\n",
    "\n",
    "### Note that prediction is the only value which changes, so that is our \"goal\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error for prediction is: 0.23990334068177915\n",
      ", With a slope of 1.404249496973327\n"
     ]
    }
   ],
   "source": [
    "# cost function to tell the computer how far out it was, lower = better\n",
    "def prediction_cost(prediction, target):\n",
    "    return (prediction - target) ** 2\n",
    "\n",
    "# getting the slope of this function, as it requires tweaking for backwards integration\n",
    "def slope_cost(prediction, target):\n",
    "    return 2 ** (prediction - target) * 1\n",
    "\n",
    "e1 = error(prediction, target)\n",
    "s1 = slope(prediction, target)\n",
    "\n",
    "print(\"The error for prediction is: {0}\".format(e1))\n",
    "print(\", With a slope of {0}\".format(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the slope into a closer depiction of the point value\n",
    "\n",
    "To do this: positive slopes require subtracting a positive float, and for negative slopes, subtracting negative floats. So, to recap, the slope function is the differential (gradient finder) for our cost function - the **cost function** being the prime way of measuring whether or not our computer is doing well in the ability to guess the colour from given data.\n",
    "\n",
    "Below is the actual training and integration of the model to find the best w1/w2/b values.\n",
    "\n",
    "**Feed forward** means to combine the weights and bias with the predefined values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2291775507539453 0.2850064793472833 -0.8719092321483759\n",
      "7.526382310815231 3.4870577682723902 -26.253997846696628\n"
     ]
    }
   ],
   "source": [
    "def random_weights():\n",
    "    return [numpy.random.randn(), numpy.random.randn(), numpy.random.randn()]\n",
    "\n",
    "def train(weights=False):\n",
    "    if weights == False:\n",
    "        w1, w2, b = numpy.random.randn(), numpy.random.randn(), numpy.random.randn()\n",
    "    else:\n",
    "        w1, w2, b = weights\n",
    "        \n",
    "    print(w1, w2, b)\n",
    "    # how drastic changes are made, 0.05 = 20% difference\n",
    "    learning_rate = 0.2\n",
    "    \n",
    "    \n",
    "    for i in range(50000):\n",
    "        # picking a flower\n",
    "        idx    = random.randint(0, len(flowers)-1)\n",
    "        point  = flowers[idx]\n",
    "        target = flowers[idx][2]\n",
    "        \n",
    "        # feeding forward, getting prediction\n",
    "        # the predicted value that we got, we get the derivative of that later on\n",
    "        z = point[0] * w1 + point[1] * w2 + b\n",
    "        prediction = sigmoid(z)\n",
    "        \n",
    "        # getting the cost and slope of pred\n",
    "        cost = (prediction - target) ** 2\n",
    "        dcost = 2 * (prediction - target)\n",
    "\n",
    "        # derivative of z-prediction, using the more-sigmoid rule since we packed it up\n",
    "        # this is the derivative of sigmoid\n",
    "        dprediction = sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "        # getting the derivatives of the individual weight/bias variables,\n",
    "        # since we are getting the derivative and lowering a level usign the\n",
    "        # power rule, we are left with:\n",
    "        # w1 * length -> length, i.e. w1 * point[0] -> point[0], 2x -> 2\n",
    "        # these are all the PARTIALS, which means what does z change by when\n",
    "        # we look at each of the individual points\n",
    "        dw1 = point[0]\n",
    "        dw2 = point[1]\n",
    "        db  = 1\n",
    "\n",
    "        # bringing the cost changes (slope) through each function\n",
    "        # through the square, then sigmoid value (backward integration)\n",
    "        # then finally through whatever is multiplying our parameter of interest (length\n",
    "        # x width) which is the last piece derivative\n",
    "        \n",
    "        # the difference in cost with respect to the randomized weights,\n",
    "        # this is the entire backwards working from cost downwards before\n",
    "        # the weights start to change, and we individually\n",
    "        # apply all values\n",
    "        d_cost_wrt_weights = dcost * 1 * dprediction * 1\n",
    "        \n",
    "        dcost_dw1 = d_cost_wrt_weights * dw1\n",
    "        dcost_dw2 = d_cost_wrt_weights * dw2\n",
    "        dcost_db  = d_cost_wrt_weights * db\n",
    "\n",
    "        # updating our parameters with the above\n",
    "        w1 -= learning_rate * dcost_dw1\n",
    "        w2 -= learning_rate * dcost_dw2\n",
    "        b  -= learning_rate * dcost_db\n",
    "        \n",
    "    print(w1, w2, b)\n",
    "    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
